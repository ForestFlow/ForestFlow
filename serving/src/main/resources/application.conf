# Copyright 2020 DreamWorks Animation L.L.C.
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
# http://www.apache.org/licenses/LICENSE-2.0
#   Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

#application.environment = "K8s" OR "local"
application.environment = ${?APPLICATION_ENVIRONMENT_CONFIG}

defaults {
  application {
    system-name = "ff-serving"
    shutdown {
      cleanup-local-dir = true
    }
    local-working-dir = ${?PWD}
    local-working-dir = ${?LOCAL_WORKING_DIR_CONFIG}
    kafka-prediction-logger.basic-topic = ${?KAFKA_PREDICTION_LOGGER_BASIC_TOPIC_CONFIG}
    kafka-prediction-logger.graphpipe-topic = ${?KAFKA_PREDICTION_LOGGER_GRAPHPIPE_TOPIC_CONFIG}

    # This should be 10 x max number of nodes expected
    max-number-of-shards = 50
    max-number-of-shards = ${?MAX_NUMBER_OF_SHARDS_CONFIG}

    # proxy and registry configs
    activity-timeout = 2 days
    activity-timeout = ${?ACTIVITY_TIMEOUT_CONFIG}

    state-snapshot-trigger   = 60 minutes
    state-snapshot-trigger   = ${?STATE_SNAPSHOT_TRIGGER_CONFIG}

    registry {
      activity-timeout = ${defaults.application.activity-timeout}
      activity-timeout = ${?REGISTRY_ACTIVITY_TIMEOUT_CONFIG}

      state-snapshot-trigger = ${defaults.application.state-snapshot-trigger}
      state-snapshot-trigger = ${?REGISTRY_STATE_SNAPSHOT_TRIGGER_CONFIG}

      # When the registry downloads a servable, it stores it in the local-working-dir
      # This controls if, upon recovery, the local copy is consulted first.
      # This can significantly speed up recovery times.
      # When used with Kubernetes, this doesn't travel well unless the local working directory is
      # backed by something like Portworx
      reuse-local-servable-copy-on-recovery = true
      reuse-local-servable-copy-on-recovery = ${?REUSE_LOCAL_SERVABLE_COPY_ON_RECOVERY_CONFIG}
    }
    proxy {
      activity-timeout = ${defaults.application.activity-timeout}
      activity-timeout = ${?PROXY_ACTIVITY_TIMEOUT_CONFIG}

      state-snapshot-trigger = ${defaults.application.state-snapshot-trigger}
      state-snapshot-trigger = ${?PROXY_STATE_SNAPSHOT_TRIGGER_CONFIG}

      # Allow enough time for commands like "create servable" to download servable/model and load into memory
      side-effects-delivery-timeout = 60 seconds
      side-effects-delivery-timeout = ${?PROXY_SIDE_EFFECTS_DELIVERY_TIMEOUT_CONFIG}
    }

    http-command-timeout = 240 seconds # IMPORTANT!
    http-command-timeout = ${?HTTP_COMMAND_TIMEOUT_CONFIG}
    http-port = 8090
    http-port = ${?APPLICATION_HTTP_PORT_CONFIG}
    http-bind-address = "0.0.0.0"
    http-bind-address = ${?APPLICATION_BIND_ADDRESS_CONFIG}
    # proxy and registry configs

    # SSL Cert Configs for Git
    ssl-verify = false
    ssl-verify = ${?SSL_VERIFY_CONFIG}
    # SSL Cert Configs for Git
  }

  akka {
    loggers = ["akka.event.slf4j.Slf4jLogger"]
    loglevel = "INFO"
    logging-filter = "akka.event.slf4j.Slf4jLoggingFilter"
    remote {
      enabled-transports = ["akka.remote.netty.tcp"]
      //      artery {
      //        enabled = off # artery didn't work on K8s for some reason
      //        # The Aeron (UDP) transport is a high performance transport and should be used for systems that require
      //        # high throughput and low latency. It uses more CPU than TCP when the system is idle or at low message rates.
      //        # There is no encryption for Aeron
      //
      //        # INFO: Can't use aeron-udp until K8s supports size limites for emptyDir volume.
      //        # "There is no limit on how much space an emptyDir or hostPath volume can consume, and no isolation between
      //        #   Containers or between Pods."
      //        #transport = aeron-udp
      //        # transport = tcp
      //      }

      # Update buffer and frame sizes to address exception of type: akka.remote.OversizedPayloadException: Discarding oversized payload sent to Actor
      netty.tcp {
        # Address issue: Transient association error (association remains live) akka.remote.OversizedPayloadException: Discarding oversized payload sent to Actor max allowed size 128000 bytes, actual size of encoded was 94438048 bytes
        # Sets the send buffer size of the Sockets,
        # set to 0b for platform default
        send-buffer-size = 600m
        send-buffer-size = ${?NETTY_SEND_BUFFER_SIZE_CONFIG}

        # Sets the receive buffer size of the Sockets,
        # set to 0b for platform default
        receive-buffer-size = 600m
        receive-buffer-size = ${?NETTY_RECEIVE_BUFFER_SIZE_CONFIG}

        # Maximum message size the transport will accept, but at least
        # 32000 bytes.
        # Please note that UDP does not support arbitrary large datagrams,
        # so this setting has to be chosen carefully when using UDP.
        # Both send-buffer-size and receive-buffer-size settings has to
        # be adjusted to be able to buffer messages of maximum size.
        maximum-frame-size = 200m
        maximum-frame-size = ${?NETTY_MAXIMUM_FRAME_SIZE_CONFIG}
      }
    }

    http.parsing {
      # Increase if you see exceptions in the form of: EntityStreamException: Request too large
      # This mostly affects batch record processing for GraphPipe requests
      max-to-strict-bytes = 200m
      max-to-strict-bytes = ${?MAX_TO_STRICT_BYTES_CONFIG}

      # Not necessarily a problem to set this to a high value.
      # Nevertheless you might want to apply some limit in order to prevent a single client from consuming
      # an excessive amount of server resources.
      max-content-length = 256m
      max-content-length = ${?MAX_CONTENT_LENGTH_CONFIG}
    }

    http.server {
      # timeouts
      request-timeout = ${defaults.application.http-command-timeout}
      request-timeout = ${?HTTP_REQUEST_TIMEOUT_CONFIG}
    }

    actor {
      provider = cluster

      # define available serializers
      serializers {
        # Java serialization (if you must)
        java = "akka.serialization.JavaSerializer"
        # Protobuf serializer
        proto = "akka.remote.serialization.ProtobufSerializer"
      }

      # map serializers to classes/traits
      enable-additional-serialization-bindings = on
      allow-java-serialization = on
      serialization-bindings {
        "scalapb.GeneratedMessage" = proto
      }
    }

    extensions = ["akka.cluster.metrics.ClusterMetricsExtension"]

    cluster.seed-nodes = []
    cluster.seed-nodes = ${?CLUSTER_SEED_NODES} // Set this if you don't want to use bootstrap discovery

    # The following 2 settings allow the node to die and K8s for example to reschedule upon an unsuccessful cluster join event SEE: https://developer.lightbend.com/docs/akka-management/current/bootstrap/index.html#recommended-configuration
    cluster.shutdown-after-unsuccessful-join-seed-nodes = 40s
    cluster.shutdown-after-unsuccessful-join-seed-nodes = ${?CLUSTER_SHUTDOWN_AFTER_UNSUCCESSFUL_JOIN_SEED_NODES_CONFIG}
    coordinated-shutdown.exit-jvm = on

    # The list of entities in each Shard can be made persistent (durable).
    # When configured to remember entities, whenever a Shard is rebalanced onto another node or recovers after a crash
    #   it will recreate all the entities which were previously running in that Shard.
    #   To permanently stop entities, a Passivate message must be sent to the parent of the entity actor, otherwise
    #   the entity will be automatically restarted after the entity restart backoff specified in the configuration.
    #
    # When rememberEntities is set to false, a Shard will not automatically restart any entities after a rebalance or
    #   recovering from a crash. Entities will only be started once the first message for that entity has been received
    #   in the Shard.
    #
    # Note that the state of the entities themselves will not be restored unless they have been made persistent.
    cluster.sharding.state-store-mode = ddata
    # This information is stored here
    cluster.sharding.distributed-data.durable.lmdb.dir = ${?defaults.local-working-dir}/sharding-distributed-data
    cluster.sharding.distributed-data.durable.lmdb.dir = ${?CLUSTER_SHARDING_DISTRIBUTED_DATA_DURABLE_LMDB_DIR_CONFIG}

    # Comment/Uncomment durable keys to disable/enabled durable storage.
    # For FF, this affects how fast a recoverd node knows of the existence of routes.
    # This is helpful because the node that crashed can still receive & serve requests for other nodes that have active
    #   contracts and servables. Typically, this should be prevented by implementing proper "readiness" checks for
    #   the node so it's not asked to serve or route any requests until it's actually fully operational.
    #   K8s can take advantage of this.
    cluster.distributed-data.durable.keys = ["contracts"]

    cluster.distributed-data.durable.lmdb.dir = ${?defaults.local-working-dir}/distributed-data
    cluster.distributed-data.durable.lmdb.dir = ${?CLUSTER_DISTRIBUTED_DATA_DURABLE_LMDB_DIR_CONFIG}
    cluster.distributed-data.durable.lmdb.write-behind-interval = 1200 ms # we are OK with this being long since crashed actors will replay ddata values anyway
    cluster.distributed-data.durable.lmdb.write-behind-interval = ${?CLUSTER_DISTRIBUTED_DATA_DURABLE_LMDB_WRITE_BEHIND_INTERVAL_CONFIG}

    cluster.downing-provider-class = com.ajjpj.simpleakkadowning.SimpleAkkaDowningProvider
    cluster.failure-detector.acceptable-heartbeat-pause = 12s # massive git clones
  }

  # https://github.com/arnohaase/simple-akka-downing
  simple-akka-downing {
    # Time margin after which shards or singletons that belonged to a downed/removed
    #  partition are created in surviving partition. The purpose of this margin is that
    #  in case of a network partition the persistent actors in the non-surviving partitions
    #  must be stopped before corresponding persistent actors are started somewhere else.
    #
    # Disable with "off" or specify a duration to enable.
    #
    # See akka.cluster.down-removal-margin
    down-removal-margin = 5s
    down-removal-margin = ${?SIMPLEAD_DOWN_REMOVAL_MARGIN_CONFIG}

    # Time margin after which unreachable nodes in a stable cluster state (i.e. no nodes changed
    #  their membership state or their reachability) are treated as permanently unreachable, and
    #  the split-brain resolution strategy kicks in.
    stable-after = 20s
    stable-after = ${?SIMPLEAD_STABLE_AFTER_CONFIG}

    # The active strategy is one of static-quorum, keep-majority and keep-oldest. It is triggered
    #  after the cluster configuration has been stable for an interval of 'stable-after'.
    #
    # static-quorum defines a fixed number of nodes, and a network partition must have at least
    #  this number of reachable nodes (in a given role, if that is specified) in order to be allowed
    #  to survive. If the quorum size is picked bigger than half the maximum number of cluster nodes,
    #  this strategy is completely robust. It does not however work well with a dynamically growing
    #  (or shrinking) cluster.
    #
    # keep-majority uses the number of cluster nodes as the baseline and requires a network partition
    #  to have more than half that number of (reachable) nodes in order to be allowed to survive. This
    #  fully supports elastically growing and shrinking clusters, but there are rare race conditions
    #  that can lead to both partitions to be downed or - potentially worse - both partitions to survive.
    #
    # keep-oldest requires the oldest member to be part of a partition for it to survive. This can be
    #  useful since the oldest node is where cluster singletons are running, so this strategy does not
    #  singletons to be migrated and restarted. It reliably prevents split brain, but it can lead to
    #  a situation where 2 nodes survive and 25 nodes are downed. To deal with the pathological special
    #  case that the oldest node is in a network partition of its own, the flag 'down-if-alone' can be
    #  used to specify the oldest node if it is all by itself.
    active-strategy = keep-majority
    active-strategy = ${?SIMPLEAD_ACTIVE_STRATEGY}

  }

  jdbc-journal {
    slick = ${defaults.slick}
    # Set logicalDelete = false if you want database entries to be permenantly deleted (not recommended).
    # Typically, you'd want another process to permenantly delete records in a controlled/time-based manner so you have
    # the opportunity to recover from fatal mistakes
    logicalDelete = true
  }

  # the akka-persistence-snapshot-store in use
  jdbc-snapshot-store {
    slick = ${defaults.slick}
  }

  # the akka-persistence-query provider in use
  jdbc-read-journal {
    slick = ${defaults.slick}
  }

  slick {
    profile = "slick.jdbc.PostgresProfile$"
    db {
      host = ${?POSTGRES_HOST_CONFIG}
      user = ${?POSTGRES_USER_CONFIG}
      database = ${?defaults.slick.db.user}
      database = ${?POSTGRES_DATABASE_CONFIG}
      port = 5432
      port = ${?POSTGRES_PORT_CONFIG}
      url = "jdbc:postgresql://"${?defaults.slick.db.host}":"${?defaults.slick.db.port}"/"${?defaults.slick.db.database}"?reWriteBatchedInserts=true"
      password = ${?POSTGRES_PASSWORD_CONFIG}
      driver = "org.postgresql.Driver"
      numThreads = 5
      maxConnections = 5
      minConnections = 1
    }
  }

  akka.management {
    cluster.bootstrap {
      new-cluster-enabled = on // turn off after initial deployment and rolling restart for safe-guard. Without new cluster formation disabled an isolated set of nodes could form a new cluster if all are restarted.
      contact-point-discovery {
        required-contact-point-nr = ${?CLUSTER_NODES_CONFIG} // INFO set this to the number of initial nodes to ensure a safe bootstrap deployment!!!!
      }
    }
  }

  # Properties for akka.kafka.ProducerSettings can be
  # defined in this section or a configuration section with
  # the same layout.
  akka.kafka.producer {
    # Tuning parameter of how many sends that can run in parallel.
    parallelism = 1000
    parallelism = ${?KAFKA_PRODUCER_SEND_PARALLELISM_CONFIG}

    # How long to wait for `KafkaProducer.close`
    close-timeout = 60s
    close-timeout = ${?KAFKA_PRODUCER_CLOSE_TIMEOUT_CONFIG}

    # Fully qualified config path which holds the dispatcher configuration
    # to be used by the producer stages. Some blocking may occur.
    # When this value is empty, the dispatcher configured for the stream
    # will be used.
    use-dispatcher = "akka.kafka.default-dispatcher"

    # The time interval to commit a transaction when using the `Transactional.sink` or `Transactional.flow`
    eos-commit-interval = 150ms # default 100ms
    eos-commit-interval = ${?KAFKA_PRODUCER_EOS_COMMIT_INTERVAL_CONFIG}

    # Properties defined by org.apache.kafka.clients.producer.ProducerConfig
    # can be defined in this configuration section.
    kafka-clients {

      compression.type = "snappy"
      compression.type = ${?KAFKA_PRODUCER_COMPRESSION_TYPE_CONFIG}

      request.timeout.ms = 2147483647
      request.timeout.ms = ${?KAFKA_PRODUCER_REQUEST_TIMEOUT_CONFIG}

      retry.backoff.ms = 500
      retry.backoff.ms = ${?KAFKA_PRODUCER_RETRY_BACKOFF_MS_CONFIG}

      bootstrap.servers = ${?KAFKA_BOOTSTRAP_SERVERS_CONFIG} // use to provide same value for both consumer and producer (Default behavior)
      bootstrap.servers = ${?KAFKA_PRODUCER_BOOTSTRAP_SERVERS_CONFIG} //  override for producer
    }
  }

  scoring-priority-mailbox {
    mailbox-type = "com.dreamworks.forestflow.akka.UnboundedControlAwareDequeBasedMailbox"
    # other mailbox configuration goes here
  }

  inverse-priority-mailbox {
    mailbox-type = "com.dreamworks.forestflow.akka.UnboundedInverseControlAwareDequeBasedMailbox"
    # other mailbox configuration goes here
  }


  # Sigar native library extract location during tests.
  # Note: use per-jvm-instance folder when running multiple jvm on one host.
  akka.cluster.metrics.native-library-extract-folder = ${user.dir}/target/native

  # Cluster Dispatcher for heartbeats
  akka.cluster.use-dispatcher = cluster-dispatcher

  cluster-dispatcher {
    type = "Dispatcher"
    executor = "fork-join-executor"
    fork-join-executor {
      parallelism-min = 2
      parallelism-max = 10
    }
  }

  blocking-io-dispatcher {
    type = Dispatcher
    executor = "thread-pool-executor"
    thread-pool-executor {
      fixed-pool-size = 32
    }
    throughput = 1
  }

  proxy-dispatcher {
    type = "Dispatcher"
    executor = "fork-join-executor"
    fork-join-executor {
      parallelism-min = 2
      parallelism-max = 10
    }
  }

}

# Rolling restarts SEE https://developer.lightbend.com/docs/akka-management/current/bootstrap/index.html#number-of-nodes-to-redeploy-at-once


local {
  #akka.log-config-on-start = on

  #remote
  akka.remote.artery.canonical.hostname = "127.0.0.1"
  akka.management.http.hostname = "127.0.0.1"
  #remote

  # akka persistence
  akka.persistence {
    // local - Setup default journal and snapshot store plugins but allow for overrides
    journal.plugin = "akka.persistence.journal.leveldb"
    journal.plugin = ${?AKKA_PERSISTENCE_JOURNAL_PLUGIN}
    journal.leveldb.dir = ${?defaults.local-working-dir}/state/journal
    journal.leveldb.dir = ${?LOCAL_AKKA_PERSISTENCE_JOURNAL_LEVELDB_DIR_CONFIG}
    snapshot-store.plugin = "akka.persistence.snapshot-store.local"
    snapshot-store.plugin = ${?AKKA_PERSISTENCE_SNAPSHOT_STORE_PLUGIN}
    snapshot-store.local.dir = ${?defaults.local-working-dir}/state/snapshots
    snapshot-store.local.dir = ${?LOCAL_AKKA_PERSISTENCE_SNAPSHOT_STORE_LOCAL_DIR_CONFIG}
  }
  # akka persistence

  # discovery-config
  akka.discovery {
    config.services = {
      local-cluster = {
        endpoints = [
          {
            host = "127.0.0.1"
            port = 8558
          }
        ]
      }
    }
  }
  # discovery-config

  # management-config
  akka.management {
    cluster.bootstrap {
      contact-point-discovery {

        service-name = "local-cluster"
        discovery-method = config

        required-contact-point-nr = 1
      }
    }
  }
  # management-config
}

K8s {
  quorum-size = 2
  quorum-size = ${?K8S_QUORUM_SIZE}
  # akka persistence
  akka.persistence {
    # Setup default journal and snapshot store plugins but allow for overrides
    // jdbc
    journal {
      plugin = "jdbc-journal"
      plugin = ${?AKKA_PERSISTENCE_JOURNAL_PLUGIN}
      // Enable the line below to automatically start the journal when the actorsystem is started
      // auto-start-journals = ["jdbc-journal"]
    }
    snapshot-store {
      plugin = "jdbc-snapshot-store"
      plugin = ${?AKKA_PERSISTENCE_SNAPSHOT_STORE_PLUGIN}
      // Enable the line below to automatically start the snapshot-store when the actorsystem is started
      // auto-start-snapshot-stores = ["jdbc-snapshot-store"]
    }
  }
  # akka persistence

  # discovery-config
  akka.discovery {
    kubernetes-api {
      pod-label-selector = "app=%s" # same as the default
      pod-label-selector = ${?K8S_POD_LABEL_SELECTOR_CONFIG}
    }
  }
  # discovery-config

  # management-config
  akka.management {
    health-checks {
      readiness-checks {
        cluster-membership = "akka.management.cluster.scaladsl.ClusterMembershipCheck"
      }
      readiness-path = "ready"
      liveness-path = "alive"
    }
    http {
      hostname = ${?HOSTNAME}
      hostname = ${?AKKA_MANAGEMENT_HOSTNAME_CONFIG}
      port = 8558
      port = ${?AKKA_MANAGEMENT_PORT_CONFIG}

      # Bind to 0.0.0.0:8558 'internally':
      bind-hostname = 0.0.0.0
      bind-hostname = ${?AKKA_MANAGEMENT_BIND_HOSTNAME_CONFIG}
      bind-port = 8558
      bind-port = ${?AKKA_MANAGEMENT_BIND_PORT_CONFIG}

      //      route-providers += "akka.management.HealthCheckRoutes"
    }
    cluster {
      health-check {
        # Valid values: "Joining", "WeaklyUp", "Up", "Leaving", "Exiting", "Down", "Removed"
        ready-states = ["Up", "WeaklyUp"]
      }
      bootstrap {
        contact-point-discovery {

          # For the kubernetes API this value is substituted into the %s in pod-label-selector
          service-name = "ff-serving"
          service-name = ${?K8S_SERVICE_NAME}
          discovery-method = kubernetes-api

          port-name = "management"

          # Get this value from the K8s deployment yaml
          required-contact-point-nr = ${K8s.quorum-size}

        }
      }
    }
  }
  # management-config
}
